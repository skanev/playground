> Coarsening the recursion, as we did in Problem 2-1 for merge sort, is a common
> way to improve the running time of quicksort in practice. We modify the base
> case of the recursion so that if the array has fewer than $k$ elements, the
> subarray is sorted by insertion sort, rather than by continued recursive calls
> to quicksort. Argue that the randomized version of this sorting algorithm runs
> in $\O(nk + n\lg(n/k))$ expected time. How should you pick $k$, both in theory
> and in practice?

In the quicksort part of the proposed algorithm, the recursion stops at level
$\lg(n/k)$, which makes the expected running time $\O(n\lg(n/k))$. However,
this leaves $n/k$ non-sorted, non-intersecting subarrays of (maximum) length
$k$.

Because of the nature of the insertion sort algorithm, it will first sort fully
one such subarray before consider the next one. Thus, it has the same
complexity as sorting each of those arrays, that is $\frac{n}{k}\O(k^2) = \O(nk).$

In theory, if we ignore the constant factors, we need to solve:

$$ n\lg{n} \ge nk + n\lg{n/k} \\\\
   \Downarrow \\\\
   \lg{n} \ge k + \lg{n} - \lg{k} \\\\
   \Downarrow \\\\
   \lg{k} \ge k $$

Which is not possible.

If we add the constant factors, we get:

$$ c_qn\lg{n} \ge c_ink + c_qn\lg(n/k) \\\\
   \Downarrow \\\\
   c_q\lg{n} \ge c_ik + c_q\lg{n} - c_q\lg{k} \\\\
   \Downarrow \\\\
   \lg{k} \ge \frac{c_i}{c_q}k $$

Which indicates that there might be a good candidate. Furthermore, the
lower-order terms should be taken into consideration too.

In practice, $k$ should be chosed by experiment.
